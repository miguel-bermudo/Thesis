\chapter{Conclusions}\label{chap:conclusions}

% guarrada para que las dos líneas queden más o menos alineadas sin contar las comillas, sorry
\chapterQuote{\hfill\textit{``Some peoples only exercise is jumping to conclusions.''}}{--- \textit{Author Unknown.}}

\vspace{1cm}

In this dissertation, we have presented SpaceRL a framework offering a set of reinforcement learning tools tailored towards Knowledge Graph completion and reasoning; a versatile tool with different available interfaces, a visualization tool to graphically display the results, a GUI  that provides access to less experienced users, and a REST API that enables for it to be operated as a MLaaS.

SpaceRL was designed not only as a tool for final users but also as a base for other developers to create their own custom tools, which can be offered as a service to a third party through its API complying with the openAPI convention. This might appeal to companies who wish to either serve or consume the capabilities offered.

Other existing proposals have merely scratched the surface when it comes to reward functions, applying unanimously terminal-based reward functions with minimal modifications and the backpropagation-focused REINFORCE algorithm in tandem.

Our novel alternative consists of a new set of reward functions and the application of RL algorithms whose potential remained unexplored. Our reward functions seek to use graph-specific information that is available before reaching the end of an episode: the distance to the answer node, and the semantic similarity to it computed from node embeddings. The implemented technique makes use of Proximal Policy Optimization and the Actor-Critic paradigm, resulting in faster training.

Both the new reward functions and policies have resulted in improvements over the state-of-the-art standard practices, particularly when using embedding-based reward functions on five widely used datasets. These results should motivate the development and evaluation of more variants of these aspects since there is a margin for improvement. Therefore, two trends of future work could be developed: 1) evaluating existing context-independent RL techniques, which are often already implemented by existing libraries but mainly remain untested in this context; 2) implementing new reward functions that make use of additional information in the graph, e.g. node attributes, which provide additional rich data.